# Minchae Rhea Kim

> **Focus**
> - Multimodal video understanding & retrieval  
> - Evaluation-driven model and system design  
> - Real-world perception under unconstrained conditions

Applied AI Scientist working on multimodal video understanding, retrieval,
and evaluation-driven system design.

I build applied multimodal AI systems that connect research-level ideas
to real-world constraints, with a focus on video understanding, retrieval,
and perception under unconstrained conditions.

My work in multimodal AI began with a cross-modal retrieval project that
learned semantic representations from images to retrieve music based on
visual cues such as color, facial expression, and symbolic objects.
This work was published as an HCI paper
(*Image is All for Music Retrieval*, DOI: 10.1080/10447318.2023.220155),
and shaped my core interest in multimodal representation learning
and retrieval systems.

Since then, I have moved closer to real-world vision AI.
At Deep Insight, I work on in-cabin multimodal perception systems (DMS/OMS),
where I analyze why models fail under real driving conditions —
including lighting variation, occlusion, pose, and sensor limitations —
and translate these insights into data-centric and model-level improvements
across RGB and infrared modalities.

Beyond evaluation, I build models and systems.
My recent work includes RGB-based rPPG modeling under noisy conditions
and video-centric side projects (QS-RAD v2, CreatePoster) that explore
multimodal video understanding, retrieval, and semantic grounding.

I am interested in roles where research-level problem definition meets
product-level constraints, particularly in multimodal video understanding,
retrieval, and reasoning.

**Links**
- LinkedIn: https://www.linkedin.com/in/themingcha  
- Email: inthemingcha [at] gmail [dot] com

